# TheSpearAndTheShieldOfLLMs

1. 거절 방식: "내부 판단" vs "외부 분류기"
결론부터 말씀드리면, 보통 두 가지를 다 씁니다. 하지만 사용자님이 지금 실습하시는 과정(SFT)은 "LLM 그 자체가 판단하게 만드는(내부 판단)" 방식입니다.

① LLM 자체 판단 (Internal Safety) - 지금 하시는 것
방식: LLM에게 "이런 질문엔 거절해"라고 SFT/RLHF로 가르쳐서, 모델이 대화 맥락을 읽고 스스로 거절 문장을 생성하게 하는 것입니다.

장점: 문맥을 이해할 수 있습니다. (예: "소설 속 폭탄 묘사"는 허용하고, "진짜 폭탄 제조법"은 거절하는 식의 뉘앙스 구분 가능)

단점: 모델이 멍청해지거나(거절만 함), 탈옥(Jailbreak) 당할 수 있습니다.

② 별도의 분류기 (External Guardrails) - 언급하신 것
방식: LLM 앞단에 **'문지기 AI(Classifier)'**를 따로 둡니다. (예: Llama Guard, OpenAI Moderation API)

작동: 사용자가 질문을 입력하면, 문지기가 먼저 "이거 위험한데?"라고 판단하면 LLM에게 아예 전달하지 않고 차단합니다.

장점: 매우 확실하고 안전합니다. LLM 파라미터를 건드리지 않아도 됩니다.

단점: 문맥 파악 능력이 떨어져서 과도하게 차단할 수 있고, 별도의 모델을 돌려야 하니 비용이 듭니다.

이유 ②: 모델의 체급 (Parameter Size) 한계
이게 사용자님이 말씀하신 부분입니다. **360M(3.6억 개)**이나 1.1B(11억 개) 모델은 최신 LLM(8B, 70B 등)에 비해 "지능"과 "용량"이 턱없이 부족합니다.

판단력 부족: 큰 모델은 *"이건 폭탄이라는 단어가 있지만, 소설 쓰기 문맥이니까 괜찮아"*라고 복합적인 추론을 할 수 있습니다. 하지만 작은 모델은 그 정도의 깊은 사고를 하기 어렵습니다.

단순 매핑 학습: 그래서 작은 모델은 복잡한 윤리적 판단보다는 "폭탄(keyword) = 죄송합니다(response)" 같은 단순한 1차원적 공식으로 학습하려는 경향이 강합니다.

Overfitting: 작은 그릇에 '안전 규칙'을 꽉 채워 넣으면, 기존에 알고 있던 '일반 대화 능력'을 밀어내 버리기 쉽습니다.
